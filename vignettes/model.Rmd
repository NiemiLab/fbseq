---
title: "`fbseq` package hierarchical model"
author: Will Landau
date: 2015
output: 
  rmarkdown::html_vignette:
    number_sections: true
    toc: true
vignette: >
  \VignetteEngine{knitr::rmarkdown}
  \VignetteIndexEntry{`heterosis` package methodology}
  \usepackage[utf8]{inputenc}
---

\providecommand{\e}{\varepsilon}
\providecommand{\nv}{{}^{-1}}
\providecommand{\ov}[1]{\overline{#1}}
\providecommand{\q}{$\quad$ \newline}
\providecommand{\rt}{\rightarrow}
\providecommand{\vc}[1]{\boldsymbol{#1}}
\providecommand{\wh}[1]{\widehat{#1}}

# Introduction

The `fbseq` package fits a hierarchical model to RNA-seq data in (nearly) fully Bayesian fashion. Although it was originally designed to fit RNA-seq data, it can also be used to analyze count data with tens of thousands of response variables and only a handful of observations each.

# The hierarchical model

Let $y_{n, g}$ be the fully preprocessed RNA-sequencing read count for library/column $n$ ($n = 1, \ldots, N)$ and gene/row $g$ ($g = 1, \ldots, G)$. Let $X$ be the $N \times L$ design matrix for gene-specific effects $\boldsymbol{\beta}_g = (\beta_{1, g}, \ \ldots, \ \beta_{L, g})$. Let $\boldsymbol{X}_n$ be the $n$'th row of $X$. Conditioned on the parameters $\boldsymbol{\beta}_{ g}$ and $\e_{n, g}$, the $y_{n, g}$'s are treated as independent and Poisson-distributed in the likelihood.

$$ \begin{align*}
y_{n,g} \ | \ \boldsymbol{\beta}_g, \ \e_{n, g} \stackrel{\text{ind}}{\sim} \text{Poisson} \left (\exp \left (\boldsymbol{X}_n \boldsymbol{\beta}_g + \e_{n, g} \right ) \right )
\end{align*} $$

## Signal

The parameters of interest are the $\beta_{\ell, g}$'s and their hyperparameters. Conditional on hyperparameters $\theta_\ell$, $\sigma_\ell^2$, and $\xi_{\ell, g}$, the $\beta_{\ell, g}$'s are independent with normal distributions.

$$ \begin{align*}
\beta_{\ell, g} \ | \ \theta_\ell, \ \sigma_\ell^2, \ \xi_{\ell, g} \stackrel{\text{ind}}{\sim} \text{Normal}(\theta_\ell, \ \sigma_\ell^2 \xi_{\ell, g})
\end{align*} $$

The design matrix $X$ should be chosen so that this independence assumption is justified as much as possible.

The prior means $\theta_\ell$ of $\beta_{\ell, 1}, \beta_{\ell, G}$ are given normal priors. Conditional on the initialization constants $c_\ell^2$, the $\theta_\ell$'s are assumed to be independent.


$$ \begin{align*}
\theta_\ell \ | \ c_\ell^2 \stackrel{\text{ind}}{\sim} \text{Normal}(0, \ c_\ell^2)
\end{align*} $$

The $c_\ell$'s should be large so that the priors on the $\theta_\ell$'s are diffuse and thus less informative than otherwise.

The $\sigma_\ell$ parameters are assumed to be independent conditional on initialization constants $s_\ell$.

$$ \begin{align*}
\sigma_\ell \ | \ s_\ell \stackrel{\text{ind}}{\sim} \text{Uniform}(0, \ s_\ell^2)
\end{align*} $$

This prior is equivalent to a $\sigma_\ell^{-1} \text{I}(\sigma_\ell < s_\ell)$ prior on $\sigma_\ell^2$. The $s_\ell$ constants should be chosen large. 


The $\xi_{\ell, g}$ parameters are a computational tool for assigning different prior distributions to the $\beta_{\ell, g}$'s. Let the the $\xi_{\ell, g}$'s be conditionally independent given constants $k_\ell$ and $r_\ell$, and let 

$$ \begin{align*}
\xi_{\ell, g} \ | \ s_\ell \stackrel{\text{ind}}{\sim} p(\xi_{\ell, g} \ | \ k_\ell, \ r_\ell)
\end{align*} $$

If $p(\xi_{\ell, g} \ | \ k_\ell, \ r_\ell) = I(\xi_{\ell, g} = 1)$, then $\beta_{\ell, g}$ have conditionally independent normal priors. If $p(\xi_{\ell, g} \ | \ k_\ell, \ r_\ell) = Exp(\text{rate} = k_\ell)$, then the $\beta_{\ell, g}$'s are independent with Laplace distributions given $\theta_\ell$ and $\sigma_\ell^2$. If $p(\xi_{\ell, g} \ | \ k_\ell, \ r_\ell) = \text{Inverse-Gamma}(k_\ell, \ r_\ell)$, then the $\beta_{\ell, g}$'s are independent with Student-$t$ distributions given $\theta_\ell$ and $\sigma_\ell^2$. If $p(\xi_{\ell, g} \ | \ k_\ell, \ r_\ell) = \text{Half-Cauchy}(0, 1)$, then the $\beta_{\ell, g}$'s are independent with horseshoe distributions given $\theta_\ell$ and $\sigma_\ell^2$. For each $\ell$ separately, the user can choose among normal, Laplace, Student-$t$, and horseshoe priors on the $\beta_{\ell, g}$'s.


## Noise

The $\e_{n, g}$ terms are noise that the Poisson distribution does not account for. Conditional on their means $\rho_n$ and variances $\gamma_g$, the $\e_{n, g}$ parameters are independent with normal distrib{}utions.

$$ \begin{align*}
\e_{n,g} \ | \ \rho_n, \gamma_g \stackrel{\text{ind}}{\sim} \text{Normal}(\rho_n, \ \gamma_g)
\end{align*} $$

The $\rho_n$'s play the role of normalization factors in RNA-seq data, accounting for library-specific effects like sequencing depth. Unfortunately, sampling the $\rho_n$'s in the MCMC causes problems, and it is recommended that the $\rho_n$'s be set constant at their starting values. This is the package default. Otherwise, the $\rho_n$'s are treated as ordinary model parameters that, conditioned on their variance $\omega^2$, are independent with normal distributions.

$$ \begin{align*}
\rho_n \ | \ \omega^2 \stackrel{\text{ind}}{\sim} \text{Normal}(0, \ \omega^2)
\end{align*} $$

$\omega$ has a uniform prior, equivalent to a truncated $\omega^{-1}$ prior on $\omega^2$.

$$ \begin{align*}
\omega \sim \text{Uniform}(0, \ w)
\end{align*} $$

The $\gamma_g$'s play the role of overdispersion terms, analogous to the overdispersions in traditional negative-binomial likelihood models used in packages like `edgeR`. Conditioned on parameters $\nu$ and $\tau$, they have independent inverse-gamma distributions.

$$ \begin{align*}
\gamma_g \ | \ \nu, \ \tau \stackrel{\text{ind}}{\sim} \text{Inverse-Gamma} \left ( \frac{\nu}{2}, \ \frac{\nu \tau}{2} \right )
\end{align*} $$

We can interpret $\nu$ as the degree to which the $\gamma_g$'s "shrink" towards $\tau$. Given the initialization constant $d$, $\nu$ has a uniform prior distribution

$$ \begin{align*}
\nu \sim \text{Uniform}(0, \ d)
\end{align*} $$

And $\tau$ is the prior center of the $\gamma_g$'s (between the prior mean and prior mode). Given initialization constants $a$ and $b$, $\tau$ has a gamma prior.

$$ \begin{align*}
\tau \sim \text{Gamma}(a, \ \text{rate} = b)
\end{align*} $$

## Model summary


$$ \begin{align*}
&y_{n,g} \ | \ \boldsymbol{\beta}_g, \ \e_{n, g} &&\stackrel{\text{ind}}{\sim} \text{Poisson} \left (\exp \left (\boldsymbol{X}_n \boldsymbol{\beta}_g + \e_{n, g} \right ) \right ) \\
&\qquad \beta_{\ell, g} \ | \ \theta_\ell, \ \sigma_\ell^2, \ \xi_{\ell, g} &&\stackrel{\text{ind}}{\sim} \text{Normal}(\theta_\ell, \ \sigma_\ell^2 \xi_{\ell, g}) \\
&\qquad \qquad \theta_\ell \ | \ c_\ell^2 &&\stackrel{\text{ind}}{\sim} \text{Normal}(0, \ c_\ell^2) \\
&\qquad \qquad \sigma_\ell \ | \ s_\ell &&\stackrel{\text{ind}}{\sim} \text{Uniform}(0, \ s_\ell^2) \\
&\qquad \qquad \xi_{\ell, g} \ | \ s_\ell &&\stackrel{\text{ind}}{\sim} p(\xi_{\ell, g} \ | \ k_\ell, \ r_\ell) \\
&\qquad \e_{n,g} \ | \ \rho_n, \gamma_g &&\stackrel{\text{ind}}{\sim} \text{Normal}(\rho_n, \ \gamma_g) \\
&\qquad \qquad \rho_n \ | \ \omega^2 &&\stackrel{\text{ind}}{\sim} \text{Normal}(0, \ \omega^2) \\
& \qquad \qquad \qquad \omega && \sim \text{Uniform}(0, \ w) \\
&\qquad \qquad \gamma_g \ | \ \nu, \ \tau &&\stackrel{\text{ind}}{\sim} \text{Inverse-Gamma} \left ( \frac{\nu}{2}, \ \frac{\nu \tau}{2} \right ) \\
&\qquad \qquad \qquad \nu &&\sim \text{Uniform}(0, \ d) \\
&\qquad \qquad \qquad \tau &&\sim \text{Gamma}(a, \ \text{rate} = b)
\end{align*} $$



# Inference

The inference is Bayesian. To explain, let $Y$ be the $N \times G$ matrix of the count data terms $y_{n, g}$. The joint posterior distribution of all the parameters given $Y$ is estimated using a slice-sampling-within-Gibbs Markov chain Monte Carlo algorithm. During the MCMC, gene-specific posterior probabilities of interest are calculated using Monte Carlo parameter samples of the $\beta_{\ell, g}$ parameters. The package can calculate posterior probabilities of logical conjunctions of inequalities involving arbitrary contrasts of the $\beta_{\ell, g}$'s. Examples of these posterior probabilities are

$$ \begin{align*}
&P\left( \left . \beta_{1, g} > 0 \  \right | \ Y \right) \\
&P\left( \left . \beta_{2, g} - \beta_{3, g} > 1 \ \right | \ Y \right) \\
&P\left( \left . \beta_{1, g} - \beta_{3, g} > 0 \text{  and  } \beta_{2, g} - \beta_{3, g} > \sqrt{2} \text{  and  } \beta_{4, g} > -\log(\pi) \ \right | \ Y \right) \\
\end{align*} $$

Any contrast can appear on the left side of each inequality, and any fixed number can appear on the right side. "Greater-than" is the only relation supported.




